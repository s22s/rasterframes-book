{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile arithmetic\n",
    "Rasterframes includes support for arithmetic operations across tiles, calling the function on the two cells that correspond to each other in tiles of the same size. This example uses localAdd, but localSubtract, localDivide, and localMultiply also exist. While not used in this example, localAddScalar, localSubtractScalar, etc can be uses to perform operations with a scalar, as opposed to the local values in another tile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import astraea.spark.rasterframes._\n",
       "import geotrellis.raster.io.geotiff.SinglebandGeoTiff\n",
       "import org.apache.spark.sql._\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5bd2d763\n",
       "readTiff: (name: String)geotrellis.raster.io.geotiff.SinglebandGeoTiff\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import astraea.spark.rasterframes._\n",
    "import geotrellis.raster.io.geotiff.SinglebandGeoTiff\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "\n",
    "implicit val spark = SparkSession.builder().\n",
    "  master(\"local\").appName(\"RasterFrames\").\n",
    "  config(\"spark.ui.enabled\", \"false\").\n",
    "  getOrCreate().\n",
    "  withRasterFrames\n",
    "\n",
    "def readTiff(name: String): SinglebandGeoTiff = SinglebandGeoTiff(s\"../samples/$name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filenamePattern: String = L8-B%d-Elkton-VA.tiff\n",
       "bandNumbers: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4)\n",
       "bandColNames: Array[String] = Array(band_1, band_2, band_3, band_4)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filenamePattern = \"L8-B%d-Elkton-VA.tiff\"\n",
    "val bandNumbers = 1 to 4\n",
    "val bandColNames = bandNumbers.map(b ⇒ s\"band_$b\").toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once imported, join the tiles together based on a `spatialKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinedRF: astraea.spark.rasterframes.RasterFrame = [spatial_key: struct<col: int, row: int>, band_1: rf_tile ... 3 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinedRF = bandNumbers.\n",
    "  map { b ⇒ (b, filenamePattern.format(b)) }.\n",
    "  map { case (b, f) ⇒ (b, readTiff(f)) }.\n",
    "  map { case (b, t) ⇒ t.projectedRaster.toRF(s\"band_$b\") }.\n",
    "  reduce(_ spatialJoin _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New columns are added based on the results of these local functions. `localAdd` adds corresponding cells, `localDivide` divides them, `localSubtractScalar` subtracts a scalar from every cell in the input tile, and `localMultiplyScalar` multiplies each cell by a certain value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val addRF = joinedRF.withColumn(\"B1+B2\", localAdd($\"band_1\", $\"band_2\")).asRF\n",
    "val divideRF = joinedRF.withColumn(\"B1/B2\", localDivide($\"band_1\", $\"band_2\")).asRF\n",
    "val subScalarRF = joinedRF.withColumn(\"B1-100\", localSubtractScalar($\"band_1\", 100)).asRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "41: error: recursive value subConstantRF needs type",
     "output_type": "error",
     "traceback": [
      "<console>:41: error: recursive value subConstantRF needs type",
      "       val subConstantRF = joinedRF.withColumn(\"B1 - C\", localSubtract($\"band_1\", subConstantRF.makeConstantTile(6, 3, 3, \"IntType\")))",
      "                                                                                  ^",
      ""
     ]
    }
   ],
   "source": [
    "val subConstantRF = joinedRF.withColumn(\"B1 - C\", localSubtract($\"band_1\", makeConstantTile(6, 3, 3, \"IntType\")))\n",
    "val multScalarRF = joinedRF.withColumn(\"B1*2\", localMultiplyScalar($\"band_1\", 2)).asRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tileMax` computes the max values of the cells in a tile, `tileSum` computes the sum of all cells, `tileMean` finds the mean value, and `tileMin` finds the minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "35: error: not found: value addRF",
     "output_type": "error",
     "traceback": [
      "<console>:35: error: not found: value addRF",
      "       addRF.select(tileMax($\"B1+B2\"), tileMax($\"band_1\"), tileMax($\"band_2\")).show()",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "addRF.select(tileMax($\"B1+B2\"), tileMax($\"band_1\"), tileMax($\"band_2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+---------------+\n",
      "|tileSum(B1/B2)|tileSum(band_1)|tileSum(band_2)|\n",
      "+--------------+---------------+---------------+\n",
      "|       31389.0|   3.09149454E8|     2.806676E8|\n",
      "+--------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "divideRF.select(tileSum($\"B1/B2\"), tileSum($\"band_1\"), tileSum($\"band_2\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "| tileMean(B1-100)| tileMean(band_1)|\n",
      "+-----------------+-----------------+\n",
      "|9734.874785264363|9834.874785264363|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subScalarRF.select(tileMean($\"B1-100\"), tileMean($\"band_1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|tileMin(B1*2)|tileMin(band_1)|\n",
      "+-------------+---------------+\n",
      "|      18522.0|         9261.0|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multScalarRF.select(tileMin($\"B1*2\"), tileMin($\"band_1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 37 in stage 43.0 failed 1 times, most recent failure: Lost task 37.0 in stage 43.0 (TID 556, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$makeConstantTile$1: () => rf_tile)",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 43.0 failed 1 times, most recent failure: Lost task 37.0 in stage 43.0 (TID 556, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$makeConstantTile$1: () => rf_tile)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:414)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "Caused by: java.lang.IllegalArgumentException: Cell type IntType is not supported",
      "\tat geotrellis.raster.CellType$.fromName(CellType.scala:441)",
      "\tat astraea.spark.rasterframes.functions.package$$anonfun$19.apply(package.scala:256)",
      "\tat astraea.spark.rasterframes.functions.package$$anonfun$19.apply(package.scala:255)",
      "\tat astraea.spark.rasterframes.RasterFunctions$$anonfun$makeConstantTile$1.apply(RasterFunctions.scala:261)",
      "\tat astraea.spark.rasterframes.RasterFunctions$$anonfun$makeConstantTile$1.apply(RasterFunctions.scala:261)",
      "\t... 16 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)",
      "  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:644)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:603)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:612)",
      "  ... 41 elided",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$makeConstantTile$1: () => rf_tile)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:414)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:108)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      "Caused by: java.lang.IllegalArgumentException: Cell type IntType is not supported",
      "  at geotrellis.raster.CellType$.fromName(CellType.scala:441)",
      "  at astraea.spark.rasterframes.functions.package$$anonfun$19.apply(package.scala:256)",
      "  at astraea.spark.rasterframes.functions.package$$anonfun$19.apply(package.scala:255)",
      "  at astraea.spark.rasterframes.RasterFunctions$$anonfun$makeConstantTile$1.apply(RasterFunctions.scala:261)",
      "  at astraea.spark.rasterframes.RasterFunctions$$anonfun$makeConstantTile$1.apply(RasterFunctions.scala:261)",
      "  ... 16 more",
      ""
     ]
    }
   ],
   "source": [
    "subConstantRF.select(tileMin($\"B1 - C\"), tileMin($\"band_1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
