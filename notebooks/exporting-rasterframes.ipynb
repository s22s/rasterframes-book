{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting&nbsp;RasterFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrasterframes import *\n",
    "from pyrasterframes.rasterfunctions import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder. \\\n",
    "    master(\"local[*]\"). \\\n",
    "    appName(\"RasterFrames\"). \\\n",
    "    config(\"spark.ui.enabled\", \"false\"). \\\n",
    "    getOrCreate(). \\\n",
    "    withRasterFrames()\n",
    "\n",
    "samplePath = 'samples/L8-B8-Robinson-IL.tiff'\n",
    "rf = spark.read.geotiff(samplePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the goal of RasterFrames is to make it as easy as possible to do your geospatial analysis with a single \n",
    "construct, it is helpful to be able to transform it into other representations for various use cases.\n",
    "\n",
    "## Converting to Array\n",
    "\n",
    "The cell values within a `Tile` are encoded internally as an array. There may be use cases \n",
    "where the additional context provided by the `Tile` construct is no longer needed and one would\n",
    "prefer to work with the underlying array data.\n",
    "\n",
    "The @scaladoc[`tileToArray`][tileToArray] column function requires a type parameter to indicate the array element\n",
    "type you would like used. The following types may be used: `Int`, `Double`, `Byte`, `Short`, `Float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tileToArray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d39d11cec49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwithArrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tileData\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtileToArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mShort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tile'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tile'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwithArrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tileToArray' is not defined"
     ]
    }
   ],
   "source": [
    "withArrays = rf.withColumn(\"tileData\", tileToArray[Short]('tile')).drop('tile')\n",
    "withArrays.show(5, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert the data back to an array, but you have to specify the target tile dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tileBack = withArrays.withColumn(\"tileAgain\", arrayToTile($\"tileData\", 128, 128))\n",
    "tileBack.drop(\"tileData\").show(5, 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the created tile will not have a `NoData` value associated with it. Here's how you can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tileBackAgain = withArrays.withColumn(\"tileAgain\", withNoData(arrayToTile($\"tileData\", 128, 128), 3))\n",
    "tileBackAgain.drop(\"tileData\").show(5, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to Parquet\n",
    "\n",
    "It is often useful to write Spark results in a form that is easily reloaded for subsequent analysis. \n",
    "The [Parquet](https://parquet.apache.org/)columnar storage format, native to Spark, is ideal for this. RasterFrames\n",
    "work just like any other DataFrame in this scenario as long as @scaladoc[`rfInit`][rfInit] is called to register\n",
    "the imagery types.\n",
    "\n",
    "\n",
    "Let's assume we have a RasterFrame we've done some fancy processing on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geotrellis.raster.equalization._\n",
    "equalizer = udf((t: Tile) => t.equalize())\n",
    "equalized = rf.withColumn(\"equalized\", equalizer($\"tile\")).asRF\n",
    "\n",
    "\n",
    "equalized.printSchema\n",
    "equalized.select(aggStats($\"tile\")).show(false)\n",
    "equalized.select(aggStats($\"equalized\")).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write it out just like any other DataFrame, including the ability to specify partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"/tmp/equalized.parquet\"\n",
    "equalized.select(\"*\", \"spatial_key.*\").write.partitionBy(\"col\", \"row\").mode(SaveMode.Overwrite).parquet(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm partitioning happened as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "new File(filePath).list.filter(f => !f.contains(\"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data back in and check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = spark.read.parquet(filePath)\n",
    "\n",
    "rf2.printSchema\n",
    "equalized.select(aggStats($\"tile\")).show(false)\n",
    "equalized.select(aggStats($\"equalized\")).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a Raster\n",
    "\n",
    "For the purposes of debugging, the RasterFrame tiles can be reassembled back into a raster for viewing. However, \n",
    "keep in mind that this will download all the data to the driver, and reassemble it in-memory. So it's not appropriate \n",
    "for very large coverages.\n",
    "\n",
    "Here's how one might render the image to a georeferenced GeoTIFF file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geotrellis.raster.io.geotiff.GeoTiff\n",
    "image = equalized.toRaster($\"equalized\", 774, 500)\n",
    "GeoTiff(image).write(\"target/scala-2.11/tut/rf-raster.tiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*Download GeoTIFF*](rf-raster.tiff)\n",
    "\n",
    "Here's how one might render a raster frame to a false color PNG file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val colors = ColorMap.fromQuantileBreaks(image.tile.histogram, ColorRamps.BlueToOrange)\n",
    "image.tile.color(colors).renderPng().write(\"target/scala-2.11/tut/rf-raster.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rf-raster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to a GeoTrellis Layer\n",
    "\n",
    "For future analysis it is helpful to persist a RasterFrame as a [GeoTrellis layer](http://geotrellis.readthedocs.io/en/latest/guide/tile-backends.html).\n",
    "\n",
    "First, convert the RasterFrame into a TileLayerRDD. The return type is an Either;\n",
    "the `left` side is for spatial-only keyed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlRDD = equalized.toTileLayerRDD($\"equalized\").left.get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a GeoTrellis layer writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.nio.file.Files\n",
    "import spray.json._\n",
    "import DefaultJsonProtocol._\n",
    "import geotrellis.spark.io._\n",
    "p = Files.createTempDirectory(\"gt-store\")\n",
    "writer: LayerWriter[LayerId] = LayerWriter(p.toUri)\n",
    "\n",
    "layerId = LayerId(\"equalized\", 0)\n",
    "writer.write(layerId, tlRDD, index.ZCurveKeyIndexMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the metadata in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttributeStore(p.toUri).readMetadata[JsValue](layerId).prettyPrint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to `RDD` and `TileLayerRDD`\n",
    "\n",
    "Since a `RasterFrame` is just a `DataFrame` with extra metadata, the method \n",
    "@scaladoc[`DataFrame.rdd`][rdd] is available for simple conversion back to `RDD` space. The type returned \n",
    "by `.rdd` is dependent upon how you select it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.runtime.universe._\n",
    "def showType[T: TypeTag](t: T) = println(implicitly[TypeTag[T]].tpe.toString)\n",
    "\n",
    "showType(rf.rdd)\n",
    "\n",
    "showType(rf.select(rf.spatialKeyColumn, $\"tile\".as[Tile]).rdd) \n",
    "\n",
    "showType(rf.select(rf.spatialKeyColumn, $\"tile\").as[(SpatialKey, Tile)].rdd) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your goal convert a single tile column with its spatial key back to a `TileLayerRDD[K]`, then there's an additional\n",
    "extension method on `RasterFrame` called [`toTileLayerRDD`][toTileLayerRDD], which preserves the tile layer metadata,\n",
    "enhancing interoperation with GeoTrellis RDD extension methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showType(rf.toTileLayerRDD($\"tile\".as[Tile]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```tut:invisible\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "[rfInit]: astraea.spark.rasterframes.package#rfInit%28SQLContext%29:Unit\n",
    "[rdd]: org.apache.spark.sql.Dataset#frdd:org.apache.spark.rdd.RDD[T]\n",
    "[toTileLayerRDD]: astraea.spark.rasterframes.RasterFrameMethods#toTileLayerRDD%28tileCol:RasterFrameMethods.this.TileColumn%29:Either[geotrellis.spark.TileLayerRDD[geotrellis.spark.SpatialKey],geotrellis.spark.TileLayerRDD[geotrellis.spark.SpaceTimeKey]]\n",
    "[tileToArray]: astraea.spark.rasterframes.ColumnFunctions#tileToArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
