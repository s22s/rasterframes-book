{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this example we will do some simple cell clustering based on multiband imagery.\n",
    "\n",
    "## Setup \n",
    "\n",
    "First some setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrasterframes import *\n",
    "from pyrasterframes.rasterfunctions import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "    master(\"local[*]\"). \\\n",
    "    appName(\"RasterFrames\"). \\\n",
    "    config(\"spark.ui.enabled\", \"false\"). \\\n",
    "    getOrCreate(). \\\n",
    "    withRasterFrames()\n",
    "    \n",
    "resource_dir = Path('./samples').resolve()\n",
    "# Utility for reading imagery from our test data set\n",
    "filenamePattern = \"L8-B{}-Elkton-VA.tiff\"\n",
    "bandNumbers = range(1, 8)\n",
    "bandColNames = list(map(lambda n: 'band_{}'.format(n), bandNumbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The first step is to load multiple bands of imagery and construct a single RasterFrame from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "joinedRF = reduce(lambda rf1, rf2: rf1.asRF().spatialJoin(rf2.drop('bounds').drop('metadata')),\n",
    "                  map(lambda bf: spark.read.geotiff(bf[1]) \\\n",
    "                      .withColumnRenamed('tile', 'band_{}'.format(bf[0])),\n",
    "                  map(lambda b: (b, resource_dir.joinpath(filenamePattern.format(b)).as_uri()), bandNumbers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see a single `spatial_key` column along with 4 columns of tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedRF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline \n",
    "\n",
    "SparkML requires that each observation be in its own row, and those\n",
    "observations be packed into a single `Vector`. The first step is to\n",
    "\"explode\" the tiles into a single row per cell/pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploder = TileExploder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To \"vectorize\" the the band columns, as required by SparkML, we use the SparkML \n",
    "`VectorAssembler`. We then configure our algorithm, create the transformation pipeline,\n",
    "and train our model. (Note: the selected value of *K* below is arbitrary.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler() \\\n",
    "    .setInputCols(bandColNames) \\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "# Configure our clustering algorithm\n",
    "k = 5\n",
    "kmeans = KMeans().setK(k)\n",
    "\n",
    "# Combine the two stages\n",
    "pipeline = Pipeline().setStages([exploder, assembler, kmeans])\n",
    "\n",
    "# Compute clusters\n",
    "model = pipeline.fit(joinedRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "At this point the model can be saved off for later use, or used immediately on the same\n",
    "data we used to compute the model. First we run the data through the model to assign \n",
    "cluster IDs to each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = model.transform(joinedRF)\n",
    "clustered.show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to inspect the model statistics, the SparkML API requires us to go\n",
    "through this unfortunate contortion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterResults = list(filter(lambda x: str(x).startswith('KMeans'), model.stages))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute sum of squared distances of points to their nearest center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = clusterResults.computeCost(clustered)\n",
    "print(\"Within set sum of squared errors: %s\" % metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "\n",
    "The predictions are in a DataFrame with each row representing a separate pixel. \n",
    "To assemble a raster to visualize the cluster assignments, we have to go through a\n",
    "multi-stage process to get the data back in tile form, and from there to combined\n",
    "raster form.\n",
    "\n",
    "First, we get the DataFrame back into RasterFrame form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = joinedRF.tileLayerMetadata()\n",
    "layout = tlm['layoutDefinition']['tileLayout']\n",
    "\n",
    "retiled = clustered.groupBy('spatial_key').agg(\n",
    "    assembleTile('column_index', 'row_index', 'prediction',\n",
    "        layout['tileCols'], layout['tileRows'], 'int8')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = retiled.asRF('spatial_key', tlm)\n",
    "rf.printSchema()\n",
    "rf.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To render our visualization, we convert to a raster first, and then use an\n",
    "`IndexedColorMap` to assign each discrete cluster a different color, and finally\n",
    "rendering to a PNG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# toRaster DNE (or at least I can't find it)\n",
    "\n",
    "raster = rf.toRaster('prediction', 186, 169)\n",
    "\n",
    "clusterColors = IndexedColorMap.fromColorMap(\n",
    "  ColorRamps.Viridis.toColorMap([range(0, k)])\n",
    ")\n",
    "\n",
    "raster.tile.renderPng(clusterColors).write(\"target/scala-2.11/tut/ml/clustered.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Color Composite    | Cluster Assignments |\n",
    "| ------------------ | ------------------- |\n",
    "| ![](L8-RGB-VA.png) | ![](clustered.png)  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
