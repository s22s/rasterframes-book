{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this example we will do some simple cell classification based on multiband imagery and a\n",
    "target/label raster. As a part of the process we'll explore the cross-validation support in\n",
    "SparkML.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First some setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrasterframes import *\n",
    "from pyrasterframes.rasterfunctions import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "    master(\"local[*]\"). \\\n",
    "    appName(\"RasterFrames\"). \\\n",
    "    config(\"spark.ui.enabled\", \"false\"). \\\n",
    "    getOrCreate(). \\\n",
    "    withRasterFrames()\n",
    "\n",
    "# Utility for reading imagery from our test data set\n",
    "resource_dir = Path('./samples').resolve()\n",
    "# Utility for reading imagery from our test data set\n",
    "filenamePattern = \"L8-B{}-Elkton-VA.tiff\"\n",
    "bandNumbers = range(1, 8)\n",
    "bandColNames = list(map(lambda n: 'band_{}'.format(n), bandNumbers))\n",
    "\n",
    "def readTiff(name):\n",
    "    return resource_dir.joinpath(filenamePattern.format(name)).as_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The first step is to load multiple bands of imagery and construct a single RasterFrame from them.\n",
    "To do this we:\n",
    "\n",
    "1. Identify the GeoTIFF filename. \n",
    "2. Read the TIFF raster\n",
    "3. Convert to a raster frame of `tileSize` sized tiles, with an appropriate column name\n",
    "4. Use the RasterFrames `spatialJoin` function to create a new RasterFrame with a column for each band\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "joinedRF = reduce(lambda rf1, rf2: rf1.asRF().spatialJoin(rf2.drop('bounds').drop('metadata')),\n",
    "                  map(lambda bf: spark.read.geotiff(bf[1]) \\\n",
    "                      .withColumnRenamed('tile', 'band_{}'.format(bf[0])),\n",
    "                  map(lambda b: (b, readTiff(b)), bandNumbers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see a single `spatial_key` column along with 6 columns of tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spatial_key: struct (nullable = false)\n",
      " |    |-- col: integer (nullable = false)\n",
      " |    |-- row: integer (nullable = false)\n",
      " |-- bounds: polygon (nullable = true)\n",
      " |-- metadata: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = false)\n",
      " |-- band_1: rf_tile (nullable = false)\n",
      " |-- band_2: rf_tile (nullable = false)\n",
      " |-- band_3: rf_tile (nullable = false)\n",
      " |-- band_4: rf_tile (nullable = false)\n",
      " |-- band_5: rf_tile (nullable = false)\n",
      " |-- band_6: rf_tile (nullable = false)\n",
      " |-- band_7: rf_tile (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedRF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly pull we pull in the target label data. When load the target label raster we have \n",
    "to convert the cell type to `Double` to meet expectations of SparkML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetCol = \"target\"\n",
    "\n",
    "target = spark.read.geotiff(readTiff(1)).withColumnRenamed('tile', targetCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a peek at what kind of label data we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------+\n",
      "|aggStats(target)                                          |\n",
      "+----------------------------------------------------------+\n",
      "|[28561,9261.0,20778.0,9834.60971254508,381596.47506968677]|\n",
      "+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target.select(aggStats(targetCol)).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the target label RasterFrame with the band tiles to create our analytics base table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = joinedRF.spatialJoin(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline\n",
    "\n",
    "The data preparation modeling pipeline is next. SparkML requires that each observation be in \n",
    "its own row, and those observations be packed into a single `Vector` type. The first step is \n",
    "to \"explode\" the tiles into a single row per cell/pixel. Then we filter out any rows that\n",
    "have `NoData` values (which will cause an error during training). Finally we use the\n",
    "SparkML `VectorAssembler` to create that `Vector`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-59b04b5a135f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-59b04b5a135f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    noDataFilter = NoDataFilter().   setInputCols(bandColNames :+ targetCol)\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "exploder = TileExploder()\n",
    "\n",
    "noDataFilter = NoDataFilter(). \\\n",
    "  setInputCols(bandColNames :+ targetCol)\n",
    "\n",
    "assembler = VectorAssembler(). \\\n",
    "  setInputCols(bandColNames). \\\n",
    "  setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a decision tree for classification. You can swap out one of the other multi-class\n",
    "classification algorithms if you like. With the algorithm selected we can assemble our modeling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = new DecisionTreeClassifier().\n",
    "  setLabelCol(targetCol).\n",
    "  setFeaturesCol(assembler.getOutputCol)\n",
    "\n",
    "pipeline = new Pipeline().\n",
    "  setStages(Array(exploder, noDataFilter, assembler, classifier))\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "To extend the sophistication of the example we are going to use the SparkML support for \n",
    "cross-validation and hyper-parameter tuning. The first step is to configure how we're \n",
    "going to evaluate our model's performance. Then we define the hyperparmeter(s) we're going to \n",
    "vary and evaluate. Finally we configure the cross validator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = new MulticlassClassificationEvaluator().\n",
    "  setLabelCol(targetCol).\n",
    "  setPredictionCol(\"prediction\").\n",
    "  setMetricName(\"accuracy\")\n",
    "\n",
    "paramGrid = new ParamGridBuilder().\n",
    "  addGrid(classifier.maxDepth, Array(2, 3, 4)).\n",
    "  build()\n",
    "\n",
    "trainer = new CrossValidator().\n",
    "  setEstimator(pipeline).\n",
    "  setEvaluator(evaluator).\n",
    "  setEstimatorParamMaps(paramGrid).\n",
    "  setNumFolds(4)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the \"go\" button:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val model = trainer.fit(abt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "To view the model's performance we format the `paramGrid` settings used for each model and \n",
    "render the parameter/performance association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.getEstimatorParamMaps.\n",
    "  map(_.toSeq.map(p â‡’ s\"${p.param.name} = ${p.value}\")).\n",
    "  map(_.mkString(\", \")).\n",
    "  zip(model.avgMetrics)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.toSeq.toDF(\"params\", \"metric\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we score the original data set (including the cells without target values) and \n",
    "add up class membership results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = model.bestModel.transform(joinedRF)\n",
    "\n",
    "scored.groupBy($\"prediction\" as \"class\").count().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "\n",
    "The predictions are in a DataFrame with each row representing a separate pixel. \n",
    "To assemble a raster to visualize the class assignments, we have to go through a\n",
    "multi-stage process to get the data back in tile form, and from there to combined\n",
    "raster form.\n",
    "\n",
    "First, we get the DataFrame back into RasterFrame form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = joinedRF.tileLayerMetadata.left.get\n",
    "\n",
    "retiled = scored.groupBy($\"spatial_key\").agg(\n",
    "  assembleTile(\n",
    "    $\"column_index\", $\"row_index\", $\"prediction\",\n",
    "    tlm.tileCols, tlm.tileRows, ByteConstantNoDataCellType\n",
    "  )\n",
    ")\n",
    "\n",
    "rf = retiled.asRF($\"spatial_key\", tlm)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To render our visualization, we convert to a raster first, and then use an\n",
    "`IndexedColorMap` to assign each discrete class a different color, and finally\n",
    "rendering to a PNG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = rf.toRaster($\"prediction\", 186, 169)\n",
    "\n",
    "clusterColors = IndexedColorMap.fromColorMap(\n",
    "  ColorRamps.Viridis.toColorMap((0 until 3).toArray)\n",
    ")\n",
    "\n",
    "raster.tile.renderPng(clusterColors).write(\"target/scala-2.11/tut/ml/classified.png\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Color Composite    | Target Labels          | Class Assignments   |\n",
    "| ------------------ | ---------------------- | ------------------- |\n",
    "| ![](L8-RGB-VA.png) | ![](target-labels.png) | ![](classified.png) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-4f87b3a3d579>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4f87b3a3d579>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ```tut:invisible\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "```tut:invisible\n",
    "raster = SinglebandGeoTiff(\"../core/src/test/resources/L8-Labels-Elkton-VA.tiff\").raster\n",
    "\n",
    "k = raster.findMinMax._2\n",
    "\n",
    "clusterColors = IndexedColorMap.fromColorMap(\n",
    "  ColorRamps.Viridis.toColorMap((0 to k).toArray)\n",
    ")\n",
    "\n",
    "raster.tile.renderPng(clusterColors).write(\"target/scala-2.11/tut/ml/target-labels.png\")\n",
    "\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
