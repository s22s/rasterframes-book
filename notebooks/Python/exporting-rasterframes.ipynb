{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting&nbsp;RasterFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrasterframes import *\n",
    "from pyrasterframes.rasterfunctions import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder. \\\n",
    "    master(\"local[*]\"). \\\n",
    "    appName(\"RasterFrames\"). \\\n",
    "    config(\"spark.ui.enabled\", \"false\"). \\\n",
    "    getOrCreate(). \\\n",
    "    withRasterFrames()\n",
    "\n",
    "samplePath = 'samples/L8-B8-Robinson-IL.tiff'\n",
    "rf = spark.read.geotiff(samplePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the goal of RasterFrames is to make it as easy as possible to do your geospatial analysis with a single \n",
    "construct, it is helpful to be able to transform it into other representations for various use cases.\n",
    "\n",
    "## Converting to Array\n",
    "\n",
    "The cell values within a `Tile` are encoded internally as an array. There may be use cases \n",
    "where the additional context provided by the `Tile` construct is no longer needed and one would\n",
    "prefer to work with the underlying array data.\n",
    "\n",
    "The `tileToIntArray` or `tileToDoubleArray` column functions can be used to create an array from tile cell values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------+\n",
      "|spatial_key|                                tiledata|\n",
      "+-----------+----------------------------------------+\n",
      "|      [2,1]|[9387, 10904, 9782, 9777, 10273, 1015...|\n",
      "|      [0,0]|[14294, 14277, 13939, 13604, 14182, 1...|\n",
      "|      [3,1]|[8498, 8423, 8550, 8603, 8561, 8685, ...|\n",
      "|      [1,0]|[9827, 9926, 10055, 9953, 9817, 10055...|\n",
      "|      [3,0]|[9651, 9600, 9442, 9179, 9181, 10513,...|\n",
      "+-----------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withArrays = rf.withColumn(\"tileData\", tileToIntArray('tile')).drop('tile')\n",
    "withArrays.select('spatial_key','tiledata').show(5, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert the data back to a tile, but you have to specify the target tile dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------+\n",
      "|spatial_key|                           tileAgain|\n",
      "+-----------+------------------------------------+\n",
      "|      [2,1]|IntRawArrayTile([I@4877bdaf,128,128)|\n",
      "|      [0,0]|IntRawArrayTile([I@76cdebbb,128,128)|\n",
      "|      [3,1]|IntRawArrayTile([I@53fb650f,128,128)|\n",
      "|      [1,0]|IntRawArrayTile([I@6cf5b081,128,128)|\n",
      "|      [3,0]|IntRawArrayTile([I@191af275,128,128)|\n",
      "+-----------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tileBack = withArrays.withColumn(\"tileAgain\", arrayToTile(\"tileData\", 128, 128))\n",
    "tileBack.drop(\"tileData\").select('spatial_key', 'tileAgain').show(5, 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the created tile will not have a `NoData` value associated with it. Here's how you can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------------------+\n",
      "|spatial_key|                                         tileAgain|\n",
      "+-----------+--------------------------------------------------+\n",
      "|      [2,1]|IntUserDefinedNoDataArrayTile([I@7610991b,128,1...|\n",
      "|      [0,0]|IntUserDefinedNoDataArrayTile([I@65add34,128,12...|\n",
      "|      [3,1]|IntUserDefinedNoDataArrayTile([I@3dc12a51,128,1...|\n",
      "|      [1,0]|IntUserDefinedNoDataArrayTile([I@84d69f1,128,12...|\n",
      "|      [3,0]|IntUserDefinedNoDataArrayTile([I@7a67e1a2,128,1...|\n",
      "+-----------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tileBackAgain = withArrays.withColumn(\"tileAgain\", withNoData(arrayToTile(\"tileData\", 128, 128), 3.0))\n",
    "tileBackAgain.drop(\"tileData\").select('spatial_key', 'tileAgain').show(5, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to Parquet\n",
    "\n",
    "It is often useful to write Spark results in a form that is easily reloaded for subsequent analysis. \n",
    "The [Parquet](https://parquet.apache.org/) columnar storage format, native to Spark, is ideal for this. RasterFrames\n",
    "work just like any other DataFrame in this scenario as long as `spark.withRasterFrames` is called to register\n",
    "the imagery types\n",
    "\n",
    "\n",
    "Let's assume we have a RasterFrame we've done some basic processing on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'localAddScalarInt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c530c9ec3f82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"plus100\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocalAddScalarInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'localAddScalarInt' is not defined"
     ]
    }
   ],
   "source": [
    "added = rf.withColumn(\"plus100\", localAddScalarInt(\"tile\", 100)).asRF()\n",
    "added.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'added' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f0d53f2ef55f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"plus100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'added' is not defined"
     ]
    }
   ],
   "source": [
    "added.select(aggStats(\"tile\")).show(1, False)\n",
    "added.select(aggStats(\"plus100\")).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write it out just like any other DataFrame, including the ability to specify partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"/tmp/equalized.parquet\"\n",
    "equalized.select(\"*\", \"spatial_key.*\").write.partitionBy(\"col\", \"row\").mode(SaveMode.Overwrite).parquet(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm partitioning happened as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "new File(filePath).list.filter(f => !f.contains(\"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data back in and check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = spark.read.parquet(filePath)\n",
    "\n",
    "rf2.printSchema\n",
    "equalized.select(aggStats($\"tile\")).show(false)\n",
    "equalized.select(aggStats($\"equalized\")).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a Raster\n",
    "\n",
    "For the purposes of debugging, the RasterFrame tiles can be reassembled back into a raster for viewing. However, \n",
    "keep in mind that this will download all the data to the driver, and reassemble it in-memory. So it's not appropriate \n",
    "for very large coverages.\n",
    "\n",
    "Here's how one might render the image to a georeferenced GeoTIFF file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geotrellis.raster.io.geotiff.GeoTiff\n",
    "image = equalized.toRaster($\"equalized\", 774, 500)\n",
    "GeoTiff(image).write(\"target/scala-2.11/tut/rf-raster.tiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*Download GeoTIFF*](rf-raster.tiff)\n",
    "\n",
    "Here's how one might render a raster frame to a false color PNG file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val colors = ColorMap.fromQuantileBreaks(image.tile.histogram, ColorRamps.BlueToOrange)\n",
    "image.tile.color(colors).renderPng().write(\"target/scala-2.11/tut/rf-raster.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rf-raster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to a GeoTrellis Layer\n",
    "\n",
    "For future analysis it is helpful to persist a RasterFrame as a [GeoTrellis layer](http://geotrellis.readthedocs.io/en/latest/guide/tile-backends.html).\n",
    "\n",
    "First, convert the RasterFrame into a TileLayerRDD. The return type is an Either;\n",
    "the `left` side is for spatial-only keyed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlRDD = equalized.toTileLayerRDD($\"equalized\").left.get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a GeoTrellis layer writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.nio.file.Files\n",
    "import spray.json._\n",
    "import DefaultJsonProtocol._\n",
    "import geotrellis.spark.io._\n",
    "p = Files.createTempDirectory(\"gt-store\")\n",
    "writer: LayerWriter[LayerId] = LayerWriter(p.toUri)\n",
    "\n",
    "layerId = LayerId(\"equalized\", 0)\n",
    "writer.write(layerId, tlRDD, index.ZCurveKeyIndexMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the metadata in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttributeStore(p.toUri).readMetadata[JsValue](layerId).prettyPrint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
