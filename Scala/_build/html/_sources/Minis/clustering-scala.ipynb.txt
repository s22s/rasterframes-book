{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this example we will do some simple cell clustering based on multiband imagery.\n",
    "\n",
    "## Setup \n",
    "\n",
    "First some setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import astraea.spark.rasterframes._\n",
       "import astraea.spark.rasterframes.ml.TileExploder\n",
       "import geotrellis.raster.io.geotiff.SinglebandGeoTiff\n",
       "import geotrellis.raster._\n",
       "import geotrellis.raster.render._\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.sql._\n",
       "readTiff: (name: String)geotrellis.raster.io.geotiff.SinglebandGeoTiff\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@65bb2f1\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import astraea.spark.rasterframes._\n",
    "import astraea.spark.rasterframes.ml.TileExploder\n",
    "import geotrellis.raster.io.geotiff.SinglebandGeoTiff\n",
    "import geotrellis.raster._\n",
    "import geotrellis.raster.render._\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "// Utility for reading imagery from our test data set\n",
    "def readTiff(name: String): SinglebandGeoTiff = SinglebandGeoTiff(s\"../samples/$name\")\n",
    "\n",
    "implicit val spark = SparkSession.builder().\n",
    "  master(\"local[*]\").appName(getClass.getName).getOrCreate().withRasterFrames\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The first step is to load multiple bands of imagery and construct a single RasterFrame from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filenamePattern: String = L8-B%d-Elkton-VA.tiff\n",
       "bandNumbers: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4)\n",
       "bandColNames: Array[String] = Array(band_1, band_2, band_3, band_4)\n",
       "joinedRF: astraea.spark.rasterframes.RasterFrame = [spatial_key: struct<col: int, row: int>, band_1: rf_tile ... 3 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filenamePattern = \"L8-B%d-Elkton-VA.tiff\"\n",
    "val bandNumbers = 1 to 4\n",
    "val bandColNames = bandNumbers.map(b ⇒ s\"band_$b\").toArray\n",
    "\n",
    "// For each identified band, load the associated image file, convert to a RasterFrame, and join\n",
    "val joinedRF = bandNumbers.\n",
    "  map { b ⇒ (b, filenamePattern.format(b)) }.\n",
    "  map { case (b, f) ⇒ (b, readTiff(f)) }.\n",
    "  map { case (b, t) ⇒ t.projectedRaster.toRF(s\"band_$b\") }.\n",
    "  reduce(_ spatialJoin _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see a single `spatial_key` column along with 4 columns of tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedRF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline \n",
    "\n",
    "SparkML requires that each observation be in its own row, and those\n",
    "observations be packed into a single `Vector`. The first step is to\n",
    "\"explode\" the tiles into a single row per cell/pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exploder: astraea.spark.rasterframes.ml.TileExploder = tile-exploder_9ea64c96fde0\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val exploder = new TileExploder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To \"vectorize\" the the band columns, as required by SparkML, we use the SparkML \n",
    "`VectorAssembler`. We then configure our algorithm, create the transformation pipeline,\n",
    "and train our model. (Note: the selected value of *K* below is arbitrary.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_cffd20eac427\n",
       "k: Int = 5\n",
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_78708a4e7fbe\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_8d53c3f74ef7\n",
       "model: org.apache.spark.ml.PipelineModel = pipeline_8d53c3f74ef7\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "  setInputCols(bandColNames).\n",
    "  setOutputCol(\"features\")\n",
    "\n",
    "// Configure our clustering algorithm\n",
    "val k = 5\n",
    "val kmeans = new KMeans().setK(k)\n",
    "\n",
    "// Combine the two stages\n",
    "val pipeline = new Pipeline().setStages(Array(exploder, assembler, kmeans))\n",
    "\n",
    "// Compute clusters\n",
    "val model = pipeline.fit(joinedRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "At this point the model can be saved off for later use, or used immediately on the same\n",
    "data we used to compute the model. First we run the data through the model to assign \n",
    "cluster IDs to each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------+-------+------+------+------+--------------------+----------+\n",
      "|spatial_key|column_index|row_index| band_1|band_2|band_3|band_4|            features|prediction|\n",
      "+-----------+------------+---------+-------+------+------+------+--------------------+----------+\n",
      "|      [0,0]|           0|        0| 9470.0|8491.0|7805.0|6697.0|[9470.0,8491.0,78...|         1|\n",
      "|      [0,0]|           1|        0| 9566.0|8607.0|8046.0|6898.0|[9566.0,8607.0,80...|         1|\n",
      "|      [0,0]|           2|        0| 9703.0|8808.0|8377.0|7222.0|[9703.0,8808.0,83...|         0|\n",
      "|      [0,0]|           3|        0| 9856.0|8983.0|8565.0|7557.0|[9856.0,8983.0,85...|         0|\n",
      "|      [0,0]|           4|        0|10105.0|9270.0|8851.0|7912.0|[10105.0,9270.0,8...|         0|\n",
      "|      [0,0]|           5|        0|10273.0|9463.0|9196.0|8341.0|[10273.0,9463.0,9...|         2|\n",
      "|      [0,0]|           6|        0| 9920.0|9077.0|8480.0|7534.0|[9920.0,9077.0,84...|         0|\n",
      "|      [0,0]|           7|        0| 9559.0|8603.0|7847.0|6829.0|[9559.0,8603.0,78...|         1|\n",
      "+-----------+------------+---------+-------+------+------+------+--------------------+----------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "clustered: org.apache.spark.sql.DataFrame = [spatial_key: struct<col: int, row: int>, column_index: int ... 7 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clustered = model.transform(joinedRF)\n",
    "clustered.show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to inspect the model statistics, the SparkML API requires us to go\n",
    "through this unfortunate contortion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusterResults: org.apache.spark.ml.clustering.KMeansModel = kmeans_78708a4e7fbe\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clusterResults = model.stages.collect{ case km: KMeansModel ⇒ km}.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute sum of squared distances of points to their nearest center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within set sum of squared errors: 1.0416215116259007E10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "metric: Double = 1.0416215116259007E10\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metric = clusterResults.computeCost(clustered)\n",
    "println(\"Within set sum of squared errors: \" + metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "\n",
    "The predictions are in a DataFrame with each row representing a separate pixel. \n",
    "To assemble a raster to visualize the cluster assignments, we have to go through a\n",
    "multi-stage process to get the data back in tile form, and from there to combined\n",
    "raster form.\n",
    "\n",
    "First, we get the DataFrame back into RasterFrame form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tlm: geotrellis.spark.TileLayerMetadata[geotrellis.spark.SpatialKey] = TileLayerMetadata(uint16raw,GridExtent(Extent(703986.502389, 4249551.61978, 709549.093643, 4254601.8671),29.90640459139769,29.883120236686878),Extent(703986.502389, 4249551.61978, 709549.093643, 4254601.8671),utm-CS,KeyBounds(SpatialKey(0,0),SpatialKey(0,0)))\n",
       "retiled: org.apache.spark.sql.DataFrame = [spatial_key: struct<col: int, row: int>, prediction: rf_tile]\n",
       "rf: astraea.spark.rasterframes.RasterFrame = [spatial_key: struct<col: int, row: int>, prediction: rf_tile]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tlm = joinedRF.tileLayerMetadata.left.get\n",
    "\n",
    "val retiled = clustered.groupBy($\"spatial_key\").agg(\n",
    "  assembleTile(\n",
    "    $\"column_index\", $\"row_index\", $\"prediction\",\n",
    "    tlm.tileCols, tlm.tileRows, ByteConstantNoDataCellType\n",
    "  )\n",
    ")\n",
    "\n",
    "val rf = retiled.asRF($\"spatial_key\", tlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To render our visualization, we convert to a raster first, and then use an\n",
    "`IndexedColorMap` to assign each discrete cluster a different color, and finally\n",
    "rendering to a PNG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raster: geotrellis.raster.ProjectedRaster[geotrellis.raster.Tile] = ProjectedRaster(Raster(CroppedTile(ByteConstantNoDataArrayTile([B@75a613f3,186,169),GridBounds(0,0,185,168)),Extent(703986.502389, 4249551.61978, 709549.093643, 4254601.8671)),utm-CS)\n",
       "clusterColors: geotrellis.raster.render.IndexedColorMap = IndexedColorMap(0x440154ff, 0x3b528bff, 0x21918cff, 0x5cc863ff, 0xfde725ff)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predRaster = rf.toRaster($\"prediction\", 186, 169)\n",
    "\n",
    "val clusterColors = IndexedColorMap.fromColorMap(\n",
    "  ColorRamps.Viridis.toColorMap((0 until k).toArray)\n",
    ")\n",
    "\n",
    "raster.tile.renderPng(clusterColors).write(\"outputs/clustered.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Color Composite    | Cluster Assignments |\n",
    "| ------------------ | ------------------- |\n",
    "| ![](outputs/L8-RGB-VA.png) | ![](outputs/clustered.png)  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
