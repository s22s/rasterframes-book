{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting&nbsp;RasterFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the usual imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrasterframes import *\n",
    "from pyrasterframes.rasterfunctions import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sparksession is decleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder. \\\n",
    "    master(\"local[*]\"). \\\n",
    "    appName(\"RasterFrames\"). \\\n",
    "    config(\"spark.ui.enabled\", \"false\"). \\\n",
    "    getOrCreate(). \\\n",
    "    withRasterFrames()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in our tiff using `spark.read.geotiff()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplePath = 'samples/L8-B8-Robinson-IL.tiff'\n",
    "rf = spark.read.geotiff(samplePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the goal of RasterFrames is to make it as easy as possible to do your geospatial analysis with a single \n",
    "construct, it is helpful to be able to transform it into other representations for various use cases.\n",
    "\n",
    "## Converting to Array\n",
    "\n",
    "The cell values within a `Tile` are encoded internally as an array. There may be use cases \n",
    "where the additional context provided by the `Tile` construct is no longer needed and one would\n",
    "prefer to work with the underlying array data.\n",
    "\n",
    "The `tileToIntArray` or `tileToDoubleArray` column functions can be used to create an array from tile cell values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------+\n",
      "|spatial_key|                                tiledata|\n",
      "+-----------+----------------------------------------+\n",
      "|      [2,1]|[9387, 10904, 9782, 9777, 10273, 1015...|\n",
      "|      [0,0]|[14294, 14277, 13939, 13604, 14182, 1...|\n",
      "|      [3,1]|[8498, 8423, 8550, 8603, 8561, 8685, ...|\n",
      "|      [1,0]|[9827, 9926, 10055, 9953, 9817, 10055...|\n",
      "|      [3,0]|[9651, 9600, 9442, 9179, 9181, 10513,...|\n",
      "+-----------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withArrays = rf.withColumn(\"tileData\", tileToIntArray('tile')).drop('tile')\n",
    "withArrays.select('spatial_key','tiledata').show(5, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the data back to a tile, but we have to specify the target tile dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------+\n",
      "|spatial_key|                           tileAgain|\n",
      "+-----------+------------------------------------+\n",
      "|      [2,1]|IntRawArrayTile([I@1791e44f,128,128)|\n",
      "|      [0,0]|IntRawArrayTile([I@14b637ba,128,128)|\n",
      "|      [3,1]|IntRawArrayTile([I@4bf62e80,128,128)|\n",
      "|      [1,0]|IntRawArrayTile([I@428eadc7,128,128)|\n",
      "|      [3,0]|IntRawArrayTile([I@5a258a45,128,128)|\n",
      "+-----------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tileBack = withArrays.withColumn(\"tileAgain\", arrayToTile(\"tileData\", 128, 128))\n",
    "tileBack.drop(\"tileData\").select('spatial_key', 'tileAgain').show(5, 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the created tile will not have a `NoData` value associated with it. Here's how you can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------------------+\n",
      "|spatial_key|                                         tileAgain|\n",
      "+-----------+--------------------------------------------------+\n",
      "|      [2,1]|IntUserDefinedNoDataArrayTile([I@41ddc27,128,12...|\n",
      "|      [0,0]|IntUserDefinedNoDataArrayTile([I@2e7267d9,128,1...|\n",
      "|      [3,1]|IntUserDefinedNoDataArrayTile([I@8141c57,128,12...|\n",
      "|      [1,0]|IntUserDefinedNoDataArrayTile([I@52b4cfa3,128,1...|\n",
      "|      [3,0]|IntUserDefinedNoDataArrayTile([I@3822cd36,128,1...|\n",
      "+-----------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tileBackAgain = withArrays.withColumn(\"tileAgain\", withNoData(arrayToTile(\"tileData\", 128, 128), 3.0))\n",
    "tileBackAgain.drop(\"tileData\").select('spatial_key', 'tileAgain').show(5, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to Parquet\n",
    "\n",
    "It is often useful to write Spark results in a form that is easily reloaded for subsequent analysis. \n",
    "The [Parquet](https://parquet.apache.org/) columnar storage format, native to Spark, is ideal for this. RasterFrames\n",
    "work just like any other DataFrame in this scenario as long as `spark.withRasterFrames` is called to register\n",
    "the imagery types\n",
    "\n",
    "\n",
    "Let's assume we have a RasterFrame we've done some basic processing on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spatial_key: struct (nullable = false)\n",
      " |    |-- col: integer (nullable = false)\n",
      " |    |-- row: integer (nullable = false)\n",
      " |-- bounds: polygon (nullable = true)\n",
      " |-- metadata: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = false)\n",
      " |-- tile: rf_tile (nullable = false)\n",
      " |-- plus100: rf_tile (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "added = rf.withColumn(\"plus100\", localAddScalarInt(\"tile\", 100)).asRF()\n",
    "added.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|aggStats(tile)                                                 |\n",
      "+---------------------------------------------------------------+\n",
      "|[388000,0,7209.0,39217.0,10160.657951030928,3315112.9759380817]|\n",
      "+---------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------+\n",
      "|aggStats(plus100)                                             |\n",
      "+--------------------------------------------------------------+\n",
      "|[388000,0,7309.0,39317.0,10260.657951030928,3315112.975938067]|\n",
      "+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "added.select(aggStats(\"tile\")).show(1, False)\n",
    "added.select(aggStats(\"plus100\")).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of cells is the same but the min, max, and mean all rose by 100. We write it out just like any other DataFrame, including the ability to specify partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SaveMode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-427ecfb68236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/tmp/equalized.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spatial_key.*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"row\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSaveMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SaveMode' is not defined"
     ]
    }
   ],
   "source": [
    "filePath = \"/tmp/equalized.parquet\"\n",
    "added.select(\"*\", \"spatial_key.*\").write.partitionBy(\"col\", \"row\").mode(SaveMode.Overwrite).parquet(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm partitioning happened as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "new File(filePath).list.filter(f => !f.contains(\"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data back in and check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = spark.read.parquet(filePath)\n",
    "\n",
    "rf2.printSchema\n",
    "equalized.select(aggStats($\"tile\")).show(false)\n",
    "equalized.select(aggStats($\"equalized\")).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a Raster\n",
    "\n",
    "For the purposes of debugging, the RasterFrame tiles can be reassembled back into a raster for viewing. However, \n",
    "keep in mind that this will download all the data to the driver, and reassemble it in-memory. So it's not appropriate \n",
    "for very large coverages.\n",
    "\n",
    "Here's how one might render the image to a georeferenced GeoTIFF file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geotrellis.raster.io.geotiff.GeoTiff\n",
    "image = equalized.toRaster($\"equalized\", 774, 500)\n",
    "GeoTiff(image).write(\"target/scala-2.11/tut/rf-raster.tiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*Download GeoTIFF*](rf-raster.tiff)\n",
    "\n",
    "Here's how one might render a raster frame to a false color PNG file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val colors = ColorMap.fromQuantileBreaks(image.tile.histogram, ColorRamps.BlueToOrange)\n",
    "image.tile.color(colors).renderPng().write(\"target/scala-2.11/tut/rf-raster.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rf-raster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to a GeoTrellis Layer\n",
    "\n",
    "For future analysis it is helpful to persist a RasterFrame as a [GeoTrellis layer](http://geotrellis.readthedocs.io/en/latest/guide/tile-backends.html).\n",
    "\n",
    "First, convert the RasterFrame into a TileLayerRDD. The return type is an Either;\n",
    "the `left` side is for spatial-only keyed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlRDD = equalized.toTileLayerRDD($\"equalized\").left.get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a GeoTrellis layer writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.nio.file.Files\n",
    "import spray.json._\n",
    "import DefaultJsonProtocol._\n",
    "import geotrellis.spark.io._\n",
    "p = Files.createTempDirectory(\"gt-store\")\n",
    "writer: LayerWriter[LayerId] = LayerWriter(p.toUri)\n",
    "\n",
    "layerId = LayerId(\"equalized\", 0)\n",
    "writer.write(layerId, tlRDD, index.ZCurveKeyIndexMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the metadata in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttributeStore(p.toUri).readMetadata[JsValue](layerId).prettyPrint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
